{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install MLflow if it's missing\n",
        "!pip install mlflow -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets from Google Drive\n",
        "base_path = '/content/drive/My Drive/dvc_storage/'\n",
        "train = pd.read_csv(base_path + 'train.csv')\n",
        "validation = pd.read_csv(base_path + 'validation.csv')\n",
        "test = pd.read_csv(base_path + 'test.csv')\n",
        "\n",
        "# Drop rows with NaNs in processed_text\n",
        "train.dropna(subset=['processed_text'], inplace=True)\n",
        "validation.dropna(subset=['processed_text'], inplace=True)\n",
        "test.dropna(subset=['processed_text'], inplace=True)\n",
        "\n",
        "print(\"NaNs dropped from datasets.\")\n",
        "\n",
        "# Now define features and targets\n",
        "X_train, y_train = train['processed_text'], train['target']\n",
        "X_val, y_val = validation['processed_text'], validation['target']\n",
        "X_test, y_test = test['processed_text'], test['target']\n",
        "\n",
        "# Define evaluation metrics\n",
        "def accuracy(preds, y):\n",
        "    return (preds == y).mean()\n",
        "\n",
        "def precision(preds, y):\n",
        "    TP = ((preds == 1) & (y == 1)).sum()\n",
        "    FP = ((preds == 1) & (y == 0)).sum()\n",
        "    return TP / (TP + FP) if (TP + FP) else 0\n",
        "\n",
        "def recall(preds, y):\n",
        "    TP = ((preds == 1) & (y == 1)).sum()\n",
        "    FN = ((preds == 0) & (y == 1)).sum()\n",
        "    return TP / (TP + FN) if TP + FN > 0 else 0\n",
        "\n",
        "def f1_score(preds, y):\n",
        "    p, r = precision(preds, y), recall(preds, y)\n",
        "    return 2 * p * r / (p + r) if p + r > 0 else 0\n",
        "\n",
        "def AUCPR(probs, y):\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(y, probs)\n",
        "    return auc(recall_curve, precision_curve)\n",
        "\n",
        "# Set MLflow Experiment\n",
        "mlflow.set_experiment(\"Spam_Classification_Models\")\n",
        "\n",
        "# Define model pipelines\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "models = {\n",
        "    \"Naive_Bayes\": make_pipeline(CountVectorizer(), MultinomialNB()),\n",
        "    \"Logistic_Regression\": make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000)),\n",
        "    \"Random_Forest\": make_pipeline(CountVectorizer(), RandomForestClassifier(random_state=42, n_jobs=-1))\n",
        "}\n",
        "\n",
        "# Run experiments\n",
        "for model_name, model_pipeline in models.items():\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Train model\n",
        "        model_pipeline.fit(X_train, y_train)\n",
        "        preds = model_pipeline.predict(X_val)\n",
        "        probs = model_pipeline.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        acc = accuracy(preds, y_val)\n",
        "        prec = precision(preds, y_val)\n",
        "        rec = recall(preds, y_val)\n",
        "        f1 = f1_score(preds, y_val)\n",
        "        precision_curve, recall_curve, _ = precision_recall_curve(y_val, probs)\n",
        "        aucpr = auc(recall_curve, precision_curve)\n",
        "\n",
        "        # Log parameters, metrics, and model\n",
        "        mlflow.log_param(\"model\", model_name)\n",
        "        mlflow.log_metric(\"Accuracy\", accuracy(preds, y_val))\n",
        "        mlflow.log_metric(\"Precision\", precision(preds, y_val))\n",
        "        mlflow.log_metric(\"Recall\", recall(preds, y_val))\n",
        "        mlflow.log_metric(\"F1_Score\", f1)\n",
        "        mlflow.log_metric(\"AUCPR\", aucpr)\n",
        "        mlflow.sklearn.log_model(model_pipeline, model_name)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"Accuracy: {accuracy(preds, y_val):.4f}\")\n",
        "        print(f\"Precision: {precision(preds, y_val):.4f}\")\n",
        "        print(f\"Recall: {recall(preds, y_val):.4f}\")\n",
        "        print(f\"F1 Score: {f1_score(preds, y_val):.4f}\")\n",
        "        print(f\"AUCPR: {aucpr:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kpvIzFMdDBz",
        "outputId": "c68fffb0-a8fa-48c0-f282-97b2d8c80f5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "NaNs dropped from datasets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/03/10 05:11:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Naive_Bayes\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9349\n",
            "Recall: 0.8778\n",
            "F1 Score: 0.9054\n",
            "AUCPR: 0.9414\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/03/10 05:11:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Logistic_Regression\n",
            "Accuracy: 0.9636\n",
            "Precision: 0.9583\n",
            "Recall: 0.7667\n",
            "F1 Score: 0.8519\n",
            "AUCPR: 0.9276\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/03/10 05:11:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Random_Forest\n",
            "Accuracy: 0.9652\n",
            "Precision: 1.0000\n",
            "Recall: 0.7444\n",
            "F1 Score: 0.8535\n",
            "AUCPR: 0.9421\n",
            "\n"
          ]
        }
      ]
    }
  ]
}