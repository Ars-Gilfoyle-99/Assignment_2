{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M9qUJawBVJe",
        "outputId": "d4780474-ceb4-4450-b358-f77add8f9ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load raw data from Google Drive\n",
        "raw_data_path = \"/content/drive/My Drive/dvc_storage/processed_sms_spam.csv\"\n",
        "data = pd.read_csv(raw_data_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install DVC and Google Support"
      ],
      "metadata": {
        "id": "vO7vW-gRSCqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dvc dvc[gdrive] -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fMyLIl9T4nv",
        "outputId": "50efa9c5-380c-43b2-f8d0-fb46aa5126e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/117.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.7/457.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.2/77.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.0/426.0 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1affZPQqT6ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import dvc.api\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset path in Google Drive\n",
        "raw_data_path = \"/content/drive/My Drive/dvc_storage/processed_sms_spam.csv\"\n",
        "data = pd.read_csv(raw_data_path)\n",
        "\n",
        "# Convert category labels to binary (spam=1, ham=0)\n",
        "data[\"target\"] = data[\"category\"].map({\"spam\": 1, \"ham\": 0})\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    regex = r\"^subject:\\s(.*)\"\n",
        "    match = re.search(regex, text)\n",
        "    if match:\n",
        "        text = match.group(1)\n",
        "    text = re.sub(r\"[^a-z .]\", \"\", text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word.isalpha() and word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "print(\"Begin text preprocessing:\", end=\"\\n\\n\")\n",
        "data[\"processed_text\"] = data[\"message\"].apply(preprocess_text)\n",
        "print(\"Preprocessing complete\")\n",
        "\n",
        "# First version (seed=42)\n",
        "train_temp, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"target\"])\n",
        "train, validation = train_test_split(train_temp, test_size=0.3, random_state=42, stratify=train_temp[\"target\"])\n",
        "\n",
        "# Save splits\n",
        "train.to_csv(\"train.csv\", index=False)\n",
        "validation.to_csv(\"validation.csv\", index=False)\n",
        "test.to_csv(\"test.csv\", index=False)\n",
        "\n",
        "# Initialize and track with DVC\n",
        "os.system(\"dvc init --subdir\")\n",
        "os.system(\"dvc remote add -d storage gdrive://1dkzdzTtGUES5kMWg9lk5N6ECtmy2a5u3\")\n",
        "os.system(\"dvc add raw_data.csv train.csv validation.csv test.csv\")\n",
        "os.system(\"git add .\")\n",
        "os.system(\"git commit -m 'Initial data split with seed=42'\")\n",
        "os.system(\"dvc push\")\n",
        "\n",
        "# Updated split (seed=100)\n",
        "train_temp_new, test_new = train_test_split(data, test_size=0.2, random_state=100, stratify=data[\"target\"])\n",
        "train_new, validation_new = train_test_split(train_temp_new, test_size=0.3, random_state=100, stratify=train_temp_new[\"target\"])\n",
        "\n",
        "# Save new splits\n",
        "train_new.to_csv(\"train.csv\", index=False)\n",
        "validation_new.to_csv(\"validation.csv\", index=False)\n",
        "test_new.to_csv(\"test.csv\", index=False)\n",
        "\n",
        "# Update DVC\n",
        "os.system(\"dvc add train.csv validation.csv test.csv\")\n",
        "os.system(\"git add .\")\n",
        "os.system(\"git commit -m 'Updated data split with seed=100'\")\n",
        "os.system(\"dvc push\")\n",
        "\n",
        "# Checkout previous version and print target distribution\n",
        "os.system(\"git checkout HEAD~1\")\n",
        "os.system(\"dvc checkout\")\n",
        "for split, file in zip([\"Train\", \"Validation\", \"Test\"], [\"train.csv\", \"validation.csv\", \"test.csv\"]):\n",
        "    df = pd.read_csv(file)\n",
        "    print(f\"{split} distribution:\")\n",
        "    print(df[\"target\"].value_counts(), \"\\n\")\n",
        "\n",
        "# Checkout latest version\n",
        "os.system(\"git checkout main\")\n",
        "os.system(\"dvc checkout\")\n",
        "for split, file in zip([\"Train\", \"Validation\", \"Test\"], [\"train.csv\", \"validation.csv\", \"test.csv\"]):\n",
        "    df = pd.read_csv(file)\n",
        "    print(f\"{split} distribution:\")\n",
        "    print(df[\"target\"].value_counts(), \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np6LkuoiWZX-",
        "outputId": "cc1b81c8-98e5-448f-9f87-58a4fd69e6f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Begin text preprocessing:\n",
            "\n",
            "Preprocessing complete\n",
            "Train distribution:\n",
            "target\n",
            "0    2701\n",
            "1     418\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Validation distribution:\n",
            "target\n",
            "0    1158\n",
            "1     180\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Test distribution:\n",
            "target\n",
            "0    966\n",
            "1    149\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Train distribution:\n",
            "target\n",
            "0    2701\n",
            "1     418\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Validation distribution:\n",
            "target\n",
            "0    1158\n",
            "1     180\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Test distribution:\n",
            "target\n",
            "0    966\n",
            "1    149\n",
            "Name: count, dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the CSVs to Google Drive"
      ],
      "metadata": {
        "id": "QDYBXZGpfC70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset path in Google Drive\n",
        "dvc_storage_path = \"/content/drive/My Drive/dvc_storage/\"\n",
        "os.makedirs(dvc_storage_path, exist_ok=True)\n",
        "\n",
        "# Save splits to Google Drive\n",
        "train.to_csv(os.path.join(dvc_storage_path, \"train.csv\"), index=False)\n",
        "validation.to_csv(os.path.join(dvc_storage_path, \"validation.csv\"), index=False)\n",
        "test.to_csv(os.path.join(dvc_storage_path, \"test.csv\"), index=False)\n",
        "\n",
        "print(\"Train, validation, and test sets saved in Google Drive at:\", dvc_storage_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbGsVgGPfGzo",
        "outputId": "5b42e7b8-6af4-47f9-b19c-1772efa9a774"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train, validation, and test sets saved in Google Drive at: /content/drive/My Drive/dvc_storage/\n"
          ]
        }
      ]
    }
  ]
}